{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/toshi/My_Breakout_A3C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toshi/anaconda3/envs/Chainer30python36/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_seeds [0 1]\n",
      "/home/toshi/My_Breakout_A3C\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Agent =  0\n",
      "observation space: Box(400, 400, 3)\n",
      "action space: Discrete(4)\n",
      "initial observation: [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "next observation: [[[11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  ...\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]]\n",
      "reward: 0.0\n",
      "done: False\n",
      "info: {}\n",
      "action; 0\n",
      "obs.shape; (4, 84, 84)\n",
      "obs[0].shape; (84, 84)\n",
      "module://ipykernel.pylab.backend_inline\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Agent =  1\n",
      "observation space: Box(400, 400, 3)\n",
      "action space: Discrete(4)\n",
      "initial observation: [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "next observation: [[[11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  ...\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]\n",
      "  [11 11 11 ... 11 11 11]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]]\n",
      "reward: 0.0\n",
      "done: False\n",
      "info: {}\n",
      "action; 3\n",
      "obs.shape; (4, 84, 84)\n",
      "obs[0].shape; (84, 84)\n",
      "module://ipykernel.pylab.backend_inline\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "# 'my_breakout_a3c_20180419'\n",
    "# A3CアルゴリズムによるOpen AI Gym自作ブロック崩し環境下での強化学習\n",
    "# Ubuntuで走らせること。Windowsではエラーになる\n",
    "# Pygameがマルチスレッドに対応していないため、PROCESSES=1では問題なくランするが、\n",
    "# PROCESSES>1にすると終了してしまう（エラーになる）。\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ['OMP_NUM_THREADS']='1'               # Numpyが複数スレッドを使用するのを防止\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import gym\n",
    "gym.undo_logger_setup()                         # Turn off gym's default logger settings\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "import chainerrl\n",
    "from chainerrl.agents import a3c\n",
    "from chainerrl import experiments\n",
    "from chainerrl import links\n",
    "from chainerrl import misc\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl.optimizers import rmsprop_async\n",
    "from chainerrl import policies\n",
    "from chainerrl.recurrent import RecurrentChainMixin\n",
    "from chainerrl import v_function\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import myenv                                    # 自作環境のインポート\n",
    "\n",
    "\n",
    "PROCESSES = 2                                   # スレッド数（プロセス数）\n",
    "# 環境変数（※注意！：環境と一致させる必要がある）\n",
    "BANDWIDTH = 4                       # ニューラルネットへの入力チャンネル数\n",
    "N_ACTIONS = 4                       # 行動の種類数（0:左移動,1:静止,2:右移動,3:ボール射出）\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "class QFunction(chainer.Chain):    \n",
    "    def __init__(self, out_size):\n",
    "        super().__init__(\n",
    "            L0=L.Convolution2D(BANDWIDTH, 32, ksize=8, stride=4),\n",
    "            L1=L.Convolution2D(32, 64, ksize=4, stride=2),\n",
    "            L2=L.Convolution2D(64, 64, ksize=3, stride=1),\n",
    "            L3=L.Linear(3136, 512),\n",
    "            L4=L.Linear(512, out_size))\n",
    " \n",
    "    def __call__(self, x, test=False):\n",
    "        h = F.relu(self.L0(x))\n",
    "        h = F.relu(self.L1(h))\n",
    "        h = F.relu(self.L2(h))\n",
    "        h = F.relu(self.L3(h))\n",
    "        h = F.tanh(self.L4(h))\n",
    "        return h\n",
    "\n",
    "\n",
    "class My_A3CFFSoftmax(chainer.ChainList, a3c.A3CModel):\n",
    "    \"\"\"My example of A3C feedforward softmax policy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pi = policies.SoftmaxPolicy(\n",
    "            model=QFunction(N_ACTIONS))\n",
    "        self.v = QFunction(1)\n",
    "        super().__init__(self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        return self.pi(state), self.v(state)\n",
    "\n",
    "\n",
    "misc.set_random_seed(0)                         # Environment threadsで使用する乱数系列の設定\n",
    "process_seeds = np.arange(PROCESSES)            # スレッド数分の乱数シードを用意\n",
    "print('process_seeds', process_seeds)\n",
    "\n",
    "\n",
    "def make_env(process_idx, test):\n",
    "    env = gym.make('myenv-v0')\n",
    "    # 当該プロセスで使用するシードを選択しenvにセット\n",
    "    process_seed = int(process_seeds[process_idx])\n",
    "    env_seed = 2**32-1- process_seed if test else process_seed\n",
    "    env.seed(env_seed)\n",
    "    if not test:\n",
    "        reward_scale_factor = 1e-2\n",
    "        misc.env_modifiers.make_reward_filtered(env, lambda x: x * reward_scale_factor)\n",
    "    return env\n",
    "\n",
    "#sample_env = gym.make('myenv-v0')\n",
    "for j in range(PROCESSES):\n",
    "    sample_env = make_env(process_seeds[j], False)\n",
    "    sample_env.seed(j)\n",
    "    obs_space = sample_env.observation_space\n",
    "    action_space = sample_env.action_space\n",
    "\n",
    "    print('Agent = ', j)\n",
    "    print('observation space:', obs_space)\n",
    "    print('action space:', action_space)\n",
    "\n",
    "    obs = sample_env.reset()\n",
    "    sample_env.render()\n",
    "    print('initial observation:', obs)\n",
    "\n",
    "    action = action_space.sample()\n",
    "    obs, r, done, info = sample_env.step(action)\n",
    "    print('next observation:', obs)\n",
    "    print('reward:', r)\n",
    "    print('done:', done)\n",
    "    print('info:', info)\n",
    "    print('action;', action)\n",
    "\n",
    "    print('obs.shape;', obs.shape)\n",
    "    print('obs[0].shape;',obs[0].shape)\n",
    "    for i in range(BANDWIDTH):\n",
    "        plt.figure()\n",
    "        plt.imshow(obs[i])\n",
    "    print(matplotlib.get_backend())\n",
    "\n",
    "\n",
    "# エージェント・モデル（ニューラルネットの設定）\n",
    "model = My_A3CFFSoftmax()\n",
    "\n",
    "# 最適化の設定（たぶんA3Cの論文にあるアルゴリズム）\n",
    "opt = rmsprop_async.RMSpropAsync(lr=7e-4, eps=0.1, alpha=0.99)  \n",
    "opt.setup(model)\n",
    "opt.add_hook(chainer.optimizer.GradientClipping(40))        # |勾配|を40以下に抑える\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = a3c.A3C(model, opt, t_max=5, gamma=0.99, beta=1e-2, phi=phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "Finished, elapsed time : 5.645029783248901 sec\n"
     ]
    }
   ],
   "source": [
    "#agent.load('myenv1000000')                       # 学習済みエージェントを使用するとき\n",
    "\n",
    "import time                                     # 時間計測の設定\n",
    "start = time.time()\n",
    "\n",
    "experiments.train_agent_async(                  # 学習訓練と評価\n",
    "    agent = agent,                              # 学習エージェント\n",
    "    outdir = 'result',                          # 結果出力ファイルの指定\n",
    "    processes = PROCESSES,                      # スレッド数（プロセス数）\n",
    "    make_env = make_env,                        # 環境\n",
    "    profile = False,                            # \n",
    "    steps = 1000000,                            # 学習ステップ数\n",
    "                                                #（エピソードとは異なるようだが、このステップ数で終了になる）\n",
    "    eval_n_runs = 10,                           # 評価時の統計回数\n",
    "    eval_interval = 50000,                      # 評価のインターバル・ステップ\n",
    "    max_episode_len = 200)                      # エピソードの長さの最大値（ここでシミュレーションを打ちきる）\n",
    "\n",
    "agent.save('myenv1000000')                       # 学習エージェントの保存（'result'にも保存されているようだ）\n",
    "\n",
    "print('Finished, elapsed time : {} sec'.format(time.time()-start))      # 処理にかかった時間の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('myenv1000000') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/toshi/myenv_mya3c\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
